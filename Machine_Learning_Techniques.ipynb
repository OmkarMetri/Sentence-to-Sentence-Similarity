{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7448604d-56cc-5eb6-bbd1-8905bf716a60"
   },
   "source": [
    "## Predict Duplicate using basic ML + NLP techniques\n",
    "\n",
    "BOW + Cosine/Euclidean/Manhattan/Jaccard/Minskowiski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e7d4aa51-afc3-ef96-11e6-f944b9bdda73"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "acafea10-9564-f25a-6849-d976ee90a51a"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"ptrain.csv\")\n",
    "df.dropna(inplace=True)\n",
    "df_train, df_test = train_test_split(df, test_size = 0.02)\n",
    "df_train.reset_index(inplace=True)\n",
    "df_test.reset_index(inplace=True)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7ff92a6d-8a72-cde1-24bb-093fb60fb452"
   },
   "source": [
    "## EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "ad969c65-aac3-f40c-4650-b50ccc143fa6"
   },
   "outputs": [],
   "source": [
    "def eda(df):\n",
    "    print (\"Duplicate Count = %s , Non Duplicate Count = %s\" \n",
    "           %(df[\"is_duplicate\"].value_counts()[1],df[\"is_duplicate\"].value_counts()[0]))\n",
    "    \n",
    "    question_ids_combined = df.qid1.tolist() + df.qid2.tolist()\n",
    "    print (\"Unique Questions = %s\" %(len(np.unique(question_ids_combined))))\n",
    "    \n",
    "    question_ids_counter = Counter(question_ids_combined)\n",
    "    sorted_question_ids_counter = sorted(question_ids_counter.items(), key=operator.itemgetter(1))\n",
    "    question_appearing_more_than_once = [i for i in question_ids_counter.values() if i > 1]\n",
    "    print (\"Count of Quesitons appearing more than once = %s\" %(len(question_appearing_more_than_once)))\n",
    "\n",
    "eda(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "607dec70-9919-5f49-3a56-0cd23c9e9670"
   },
   "source": [
    "## Train Dictionary \n",
    "\n",
    "Filter extremes to remove words appearing less than 5 times in the corpus or in more than 80% of the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "9eb1f3bb-0ba9-bef3-487b-1ad5a440abf9"
   },
   "outputs": [],
   "source": [
    "words = re.compile(r\"\\w+\",re.I)\n",
    "stopword = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize_questions(df):\n",
    "    question_1_tokenized = []\n",
    "    question_2_tokenized = []\n",
    "\n",
    "    for q in df.question1.tolist():\n",
    "        question_1_tokenized.append([stemmer.stem(i.lower()) for i in words.findall(q) if i not in stopword])\n",
    "\n",
    "    for q in df.question2.tolist():\n",
    "        question_2_tokenized.append([stemmer.stem(i.lower()) for i in words.findall(q) if i not in stopword])\n",
    "\n",
    "    df[\"Question_1_tok\"] = question_1_tokenized\n",
    "    df[\"Question_2_tok\"] = question_2_tokenized\n",
    "    \n",
    "    return df\n",
    "\n",
    "def train_dictionary(df):\n",
    "    \n",
    "    questions_tokenized = df.Question_1_tok.tolist() + df.Question_2_tok.tolist()\n",
    "    \n",
    "    dictionary = corpora.Dictionary(questions_tokenized)\n",
    "    dictionary.filter_extremes(no_below=5, no_above=0.8)\n",
    "    dictionary.compactify()\n",
    "    \n",
    "    return dictionary\n",
    "    \n",
    "df_train = tokenize_questions(df_train)\n",
    "dictionary = train_dictionary(df_train)\n",
    "print (\"No of words in the dictionary = %s\" %len(dictionary.token2id))\n",
    "\n",
    "df_test = tokenize_questions(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "56bed0e8-b345-d2cd-3816-1748b47c4483"
   },
   "source": [
    "## Vector creation\n",
    "Here we are using the simple method of Bag Of Words Technique to convert sentences into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c6c05814-8314-8af4-cabc-b39c51a58293"
   },
   "outputs": [],
   "source": [
    "def get_vectors(df, dictionary):\n",
    "    \n",
    "    question1_vec = [dictionary.doc2bow(text) for text in df.Question_1_tok.tolist()]\n",
    "    question2_vec = [dictionary.doc2bow(text) for text in df.Question_2_tok.tolist()]\n",
    "    \n",
    "    question1_csc = gensim.matutils.corpus2csc(question1_vec, num_terms=len(dictionary.token2id))\n",
    "    question2_csc = gensim.matutils.corpus2csc(question2_vec, num_terms=len(dictionary.token2id))\n",
    "    \n",
    "    return question1_csc.transpose(),question2_csc.transpose()\n",
    "\n",
    "\n",
    "q1_csc, q2_csc = get_vectors(df_train, dictionary)\n",
    "\n",
    "print (q1_csc.shape)\n",
    "print (q2_csc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "113ed18c-6634-8ba2-f693-2521cfba2ac7"
   },
   "source": [
    "## Similarity Calculation Functions\n",
    "\n",
    "Here we have defined various Distance calculation functions for \n",
    " - Cosine Distance\n",
    " - Euclidean Distance\n",
    " - Manhattan Distance\n",
    " - Jaccard Distance\n",
    " - Minkowski Distance\n",
    "\n",
    "As Eucledian, Manhattan and Minkowski Distance may go beyond 1 we have scaled them from 0 - 1 , for that we are using MinMaxScaler and training them on training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8b90d4fd-5c98-9f58-3e6e-1ed968b6f869"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
    "from sklearn.metrics.pairwise import manhattan_distances as md\n",
    "from sklearn.metrics.pairwise import euclidean_distances as ed\n",
    "from sklearn.metrics import jaccard_similarity_score as jsc\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minkowski_dis = DistanceMetric.get_metric('minkowski')\n",
    "mms_scale_man = MinMaxScaler()\n",
    "mms_scale_euc = MinMaxScaler()\n",
    "mms_scale_mink = MinMaxScaler()\n",
    "\n",
    "def get_similarity_values(q1_csc, q2_csc):\n",
    "    cosine_sim = []\n",
    "    manhattan_dis = []\n",
    "    eucledian_dis = []\n",
    "    jaccard_dis = []\n",
    "    minkowsk_dis = []\n",
    "    \n",
    "    for i,j in zip(q1_csc, q2_csc):\n",
    "        sim = cs(i,j)\n",
    "        cosine_sim.append(sim[0][0])\n",
    "        sim = md(i,j)\n",
    "        manhattan_dis.append(sim[0][0])\n",
    "        sim = ed(i,j)\n",
    "        eucledian_dis.append(sim[0][0])\n",
    "        i_ = i.toarray()\n",
    "        j_ = j.toarray()\n",
    "        try:\n",
    "            sim = jsc(i_,j_)\n",
    "            jaccard_dis.append(sim)\n",
    "        except:\n",
    "            jaccard_dis.append(0)\n",
    "            \n",
    "        sim = minkowski_dis.pairwise(i_,j_)\n",
    "        minkowsk_dis.append(sim[0][0])\n",
    "    \n",
    "    return cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis    \n",
    "\n",
    "# cosine_sim = get_cosine_similarity(q1_csc, q2_csc)\n",
    "cosine_sim, manhattan_dis, eucledian_dis, jaccard_dis, minkowsk_dis = get_similarity_values(q1_csc, q2_csc)\n",
    "print (\"cosine_sim sample= \\n\", cosine_sim[0:2])\n",
    "print (\"manhattan_dis sample = \\n\", manhattan_dis[0:2])\n",
    "print (\"eucledian_dis sample = \\n\", eucledian_dis[0:2])\n",
    "print (\"jaccard_dis sample = \\n\", jaccard_dis[0:2])\n",
    "print (\"minkowsk_dis sample = \\n\", minkowsk_dis[0:2])\n",
    "\n",
    "eucledian_dis_array = np.array(eucledian_dis).reshape(-1,1)\n",
    "manhattan_dis_array = np.array(manhattan_dis).reshape(-1,1)\n",
    "minkowsk_dis_array = np.array(minkowsk_dis).reshape(-1,1)\n",
    "    \n",
    "manhattan_dis_array = mms_scale_man.fit_transform(manhattan_dis_array)\n",
    "eucledian_dis_array = mms_scale_euc.fit_transform(eucledian_dis_array)\n",
    "minkowsk_dis_array = mms_scale_mink.fit_transform(minkowsk_dis_array)\n",
    "\n",
    "eucledian_dis = eucledian_dis_array.flatten()\n",
    "manhattan_dis = manhattan_dis_array.flatten()\n",
    "minkowsk_dis = minkowsk_dis_array.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ef529f38-e1c5-4c01-f250-65b37e8efc62"
   },
   "source": [
    "## Calculate Log Loss\n",
    "\n",
    "Here we will use log loss formula to set a base criteria as to what accuracy our algorithm is able to achieve in terms of log loss which is the competition calucation score.\n",
    "\n",
    "We will also use Eucledian, Manhattan , Minkowski and Jaccard to calculate the similarity and then have a look at the log loss from each one of them. These are the five most widely used similarity classes used in Data Science so Lets use each one of them to see which performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8c4e9eb2-f52a-950e-8ddb-14110f64ba7b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "def calculate_logloss(y_true, y_pred):\n",
    "    loss_cal = log_loss(y_true, y_pred)\n",
    "    return loss_cal\n",
    "\n",
    "q1_csc_test, q2_csc_test = get_vectors(df_test, dictionary)\n",
    "y_pred_cos, y_pred_man, y_pred_euc, y_pred_jac, y_pred_mink = get_similarity_values(q1_csc_test, q2_csc_test)\n",
    "y_true = df_test.is_duplicate.tolist()\n",
    "\n",
    "y_pred_man_array = mms_scale_man.transform(np.array(y_pred_man).reshape(-1,1))\n",
    "y_pred_man = y_pred_man_array.flatten()\n",
    "\n",
    "y_pred_euc_array = mms_scale_euc.transform(np.array(y_pred_euc).reshape(-1,1))\n",
    "y_pred_euc = y_pred_euc_array.flatten()\n",
    "\n",
    "y_pred_mink_array = mms_scale_mink.transform(np.array(y_pred_mink).reshape(-1,1))\n",
    "y_pred_mink = y_pred_mink_array.flatten()\n",
    "\n",
    "logloss = calculate_logloss(y_true, y_pred_cos)\n",
    "print (\"The calculated log loss value on the test set for cosine sim is = %f\" %logloss)\n",
    "\n",
    "logloss = calculate_logloss(y_true, y_pred_man)\n",
    "print (\"The calculated log loss value on the test set for manhattan sim is = %f\" %logloss)\n",
    "\n",
    "logloss = calculate_logloss(y_true, y_pred_euc)\n",
    "print (\"The calculated log loss value on the test set for euclidean sim is = %f\" %logloss)\n",
    "\n",
    "logloss = calculate_logloss(y_true, y_pred_jac)\n",
    "print (\"The calculated log loss value on the test set for jaccard sim is = %f\" %logloss)\n",
    "\n",
    "logloss = calculate_logloss(y_true, y_pred_mink)\n",
    "print (\"The calculated log loss value on the test set for minkowski sim is = %f\" %logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "8022d9e6-5b04-9473-821c-7df290040fe4"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "X_train = pd.DataFrame({\"cos\" : cosine_sim, \"man\" : manhattan_dis, \"euc\" : eucledian_dis, \"jac\" : jaccard_dis, \"min\" : minkowsk_dis})\n",
    "y_train = df_train.is_duplicate\n",
    "\n",
    "X_test = pd.DataFrame({\"cos\" : y_pred_cos, \"man\" : y_pred_man, \"euc\" : y_pred_euc, \"jac\" : y_pred_jac, \"min\" : y_pred_mink})\n",
    "y_test = y_true\n",
    "\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(X_train, y_train)\n",
    "\n",
    "svr = SVR()\n",
    "svr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "77cc19b8-a2a0-a3c3-2f63-df0496fdba52"
   },
   "outputs": [],
   "source": [
    "y_rfr_predicted = rfr.predict(X_test)\n",
    "y_svr_predicted = svr.predict(X_test)\n",
    "\n",
    "logloss_rfr = calculate_logloss(y_test, y_rfr_predicted)\n",
    "logloss_svr = calculate_logloss(y_test, y_svr_predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The calculated log loss value on the test set using RFR is = 2.330000\n",
      "The calculated log loss value on the test set using SVR is = 2.010000\n"
     ]
    }
   ],
   "source": [
    "print (\"The calculated log loss value on the test set using RFR is = %f\" %logloss_rfr)\n",
    "print (\"The calculated log loss value on the test set using SVR is = %f\" %logloss_svr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 1,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
