{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "11f68ab6-9ffa-fec5-3f76-2d351c236692"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5fa9104a-6208-4724-27dc-dd7ad4b48272"
   },
   "source": [
    "## Read Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "eca0de69-55e1-1cd8-137a-537e4d2d4c10"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('ptrain.csv', encoding='utf-8')\n",
    "df_train['id'] = df_train['id'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "5c77b206-7b3c-c59b-e6d9-f84cb1eed3c5"
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('ptest.csv', encoding='utf-8')\n",
    "df_test['test_id'] = df_test['test_id'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "55e0097d-08c1-5bcf-42f4-32240ee34fbb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/manjunath/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df_all = pd.concat((df_train, df_test))\n",
    "df_all['question1'].fillna('', inplace=True)\n",
    "df_all['question2'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(\"concat.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "94aaae1c-c081-b41d-d1fc-e8091406c9f6"
   },
   "source": [
    "## Create Vocab ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "eb60bcdc-0345-d72b-ebe2-611a964b8727"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "0e2695cb-4e6f-e7af-4087-82acfd8229b2"
   },
   "outputs": [],
   "source": [
    "counts_vectorizer = CountVectorizer(max_features=10000-1).fit( \\\n",
    "                    itertools.chain(df_all['question1'], df_all['question2']))\n",
    "other_index = len(counts_vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f1114921-8527-437c-c980-30a04a0a9108"
   },
   "source": [
    "##Prep Data##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "ca2f2087-8f6c-787e-5042-c5e379c1a131"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "f22db32e-2d45-65fb-6847-59ad7020f854"
   },
   "outputs": [],
   "source": [
    "words_tokenizer = re.compile(counts_vectorizer.token_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "1bafb51f-d0c1-1342-dc80-9225b028beec"
   },
   "outputs": [],
   "source": [
    "def create_padded_seqs(texts, max_len=10):\n",
    "    seqs = texts.apply(lambda s: \n",
    "        [counts_vectorizer.vocabulary_[w] if w in counts_vectorizer.vocabulary_ else other_index\n",
    "         for w in words_tokenizer.findall(s.lower())])\n",
    "    return pad_sequences(seqs, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "8afdab6e-f05c-4e6c-0d6d-e395861b5d4e"
   },
   "outputs": [],
   "source": [
    "X1_train, X1_val, X2_train, X2_val, y_train, y_val = \\\n",
    "    train_test_split(create_padded_seqs(df_all[df_all['id'].notnull()]['question1']), \n",
    "                     create_padded_seqs(df_all[df_all['id'].notnull()]['question2']),\n",
    "                     df_all[df_all['id'].notnull()]['is_duplicate'].values,\n",
    "                     stratify=df_all[df_all['id'].notnull()]['is_duplicate'].values,\n",
    "                     test_size=0.3, random_state=1989)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "7e5e7c17-a66a-acd8-3bc2-a5b4c643f5b4"
   },
   "source": [
    "##Training##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "b9da33a1-8f1c-c6a2-e1e7-40656be2cbe7"
   },
   "outputs": [],
   "source": [
    "import keras.layers as lyr\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "1becec26-2df1-b38c-afc5-6a053ab47382"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/manjunath/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 10)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 10, 100)      1000000     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 256)          365568      embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 256)          0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 16)           4112        multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 1)            17          dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,369,697\n",
      "Trainable params: 1,369,697\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input1_tensor = lyr.Input(X1_train.shape[1:])\n",
    "input2_tensor = lyr.Input(X2_train.shape[1:])\n",
    "\n",
    "words_embedding_layer = lyr.Embedding(X1_train.max() + 1, 100)\n",
    "seq_embedding_layer = lyr.LSTM(256, activation='tanh')\n",
    "\n",
    "seq_embedding = lambda tensor: seq_embedding_layer(words_embedding_layer(tensor))\n",
    "\n",
    "merge_layer = lyr.multiply([seq_embedding(input1_tensor), seq_embedding(input2_tensor)])\n",
    "\n",
    "dense1_layer = lyr.Dense(16, activation='sigmoid')(merge_layer)\n",
    "ouput_layer = lyr.Dense(1, activation='sigmoid')(dense1_layer)\n",
    "\n",
    "model = Model([input1_tensor, input2_tensor], ouput_layer)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "852640d6-6318-2cf6-1b1e-6409f36ec951"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/manjunath/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 283000 samples, validate on 121287 samples\n",
      "Epoch 1/1\n",
      " - 294s - loss: 0.4984 - val_loss: 0.4605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8e797ef7f0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X1_train, X2_train], y_train, \n",
    "          validation_data=([X1_val, X2_val], y_val), \n",
    "          batch_size=128, epochs=1, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b5d7e836-cd1a-05cb-7741-d967a2bf34a1"
   },
   "source": [
    "##Extract Features From Model##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "37dbfa10-9aa3-fe0d-95f2-8b1f2aeda32f"
   },
   "outputs": [],
   "source": [
    "features_model = Model([input1_tensor, input2_tensor], merge_layer)\n",
    "features_model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "a59d052f-1eca-7693-cafe-ff4b899e469b"
   },
   "outputs": [],
   "source": [
    "F_train = features_model.predict([X1_train, X2_train], batch_size=128)\n",
    "F_val = features_model.predict([X1_val, X2_val], batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d47649ca-f55a-1528-86a1-bc465b85189d"
   },
   "source": [
    "##Train XGBoost##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "a9e938f7-aa89-a0e0-298c-721763c198ae"
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "46f28c7b-318e-fb0c-49d3-fa91b1967086"
   },
   "outputs": [],
   "source": [
    "dTrain = xgb.DMatrix(F_train, label=y_train)\n",
    "dVal = xgb.DMatrix(F_val, label=y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "36498229-1969-8424-79c9-6e7e9ddd7ba3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.653118\tval-logloss:0.656381\n",
      "Multiple eval metrics have been passed: 'val-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until val-logloss hasn't improved in 10 rounds.\n",
      "[10]\ttrain-logloss:0.469503\tval-logloss:0.496619\n",
      "[20]\ttrain-logloss:0.418184\tval-logloss:0.460799\n",
      "[30]\ttrain-logloss:0.398081\tval-logloss:0.452042\n",
      "[40]\ttrain-logloss:0.387269\tval-logloss:0.449608\n",
      "[50]\ttrain-logloss:0.37948\tval-logloss:0.448558\n",
      "[60]\ttrain-logloss:0.373128\tval-logloss:0.447942\n",
      "[70]\ttrain-logloss:0.367407\tval-logloss:0.447252\n",
      "[80]\ttrain-logloss:0.36202\tval-logloss:0.44676\n",
      "[90]\ttrain-logloss:0.357376\tval-logloss:0.446459\n",
      "[100]\ttrain-logloss:0.352835\tval-logloss:0.446073\n",
      "[110]\ttrain-logloss:0.349039\tval-logloss:0.44574\n",
      "[120]\ttrain-logloss:0.345726\tval-logloss:0.445452\n",
      "[130]\ttrain-logloss:0.341884\tval-logloss:0.445249\n",
      "[140]\ttrain-logloss:0.33845\tval-logloss:0.445006\n",
      "[150]\ttrain-logloss:0.335255\tval-logloss:0.444818\n",
      "[160]\ttrain-logloss:0.332124\tval-logloss:0.44464\n",
      "[170]\ttrain-logloss:0.328989\tval-logloss:0.444477\n",
      "[180]\ttrain-logloss:0.326158\tval-logloss:0.444388\n",
      "[190]\ttrain-logloss:0.323113\tval-logloss:0.444103\n",
      "[200]\ttrain-logloss:0.320509\tval-logloss:0.443912\n",
      "[210]\ttrain-logloss:0.317594\tval-logloss:0.44378\n",
      "[220]\ttrain-logloss:0.315146\tval-logloss:0.443675\n",
      "[230]\ttrain-logloss:0.312626\tval-logloss:0.44348\n",
      "[240]\ttrain-logloss:0.309862\tval-logloss:0.44342\n",
      "Stopping. Best iteration:\n",
      "[238]\ttrain-logloss:0.310234\tval-logloss:0.44338\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'booster': 'gbtree',\n",
    "    'eval_metric': 'logloss',\n",
    "    'eta': 0.1, \n",
    "    'max_depth': 9,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 1 / F_train.shape[1]**0.5,\n",
    "    'min_child_weight': 5,\n",
    "    'silent': 1\n",
    "}\n",
    "bst = xgb.train(xgb_params, dTrain, 1000,  [(dTrain,'train'), (dVal,'val')], \n",
    "                verbose_eval=10, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "eb0f298c-7513-d3df-0947-aa8ca6d5bd87"
   },
   "source": [
    "##Predict Test##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "a45ad89e-d81d-26a5-48e3-4c7e7487dd5a"
   },
   "outputs": [],
   "source": [
    "X1_test = create_padded_seqs(df_all[df_all['test_id'].notnull()]['question1'])\n",
    "X2_test = create_padded_seqs(df_all[df_all['test_id'].notnull()]['question2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "7ebf03ed-1ccf-22f5-c296-1b307adb5de8"
   },
   "outputs": [],
   "source": [
    "F_test = features_model.predict([X1_test, X2_test], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "226069f1-e7e6-2559-593f-80b7326e7c1c"
   },
   "outputs": [],
   "source": [
    "dTest = xgb.DMatrix(F_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "832684e5-e1d1-235b-bcfc-52e0c19832fa"
   },
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame({\n",
    "        'test_id': df_all[df_all['test_id'].notnull()]['test_id'].values,\n",
    "        'is_duplicate': bst.predict(dTest, ntree_limit=bst.best_ntree_limit)\n",
    "    }).set_index('test_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "119ad0de-8aea-b97a-7816-10b671d8fbc9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>test_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.119859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.044689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.667651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.571104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.423248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         is_duplicate\n",
       "test_id              \n",
       "0            0.119859\n",
       "1            0.044689\n",
       "2            0.667651\n",
       "3            0.571104\n",
       "4            0.423248"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_cell_guid": "f48ad5db-a1d7-b97a-8052-269e7ce43160"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id  target\n",
       "0        0       1\n",
       "1        1       1\n",
       "2        2       1\n",
       "3        3       1\n",
       "4        4       1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv(\"submissions.csv\")\n",
    "df1 = df1.iloc[:200000,:]\n",
    "df1 = df1.rename(index=str, columns={\"test_id\": \"test_id\", \"is_duplicate\": \"target\"})\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.693535\n"
     ]
    }
   ],
   "source": [
    "predicted = [0 if x < 0.05 else 1 for x in df_sub[\"is_duplicate\"].values]\n",
    "actual = df1[\"target\"].values\n",
    "res = [1 for i in range(len(predicted)) if predicted[i] == actual[i]]\n",
    "print(np.sum(res)/len(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.208124678164954\n"
     ]
    }
   ],
   "source": [
    "def logloss(ptest):\n",
    "    s = 0\n",
    "    for res in ptest:\n",
    "        s+=np.log(res)\n",
    "    return -s\n",
    "\n",
    "print(logloss(df_sub[\"is_duplicate\"])/len(df_sub[\"is_duplicate\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
